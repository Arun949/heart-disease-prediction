# Heart Disease Prediction Project ğŸ«€

A comprehensive machine learning project for predicting heart disease probability using ensemble learning techniques.

## ğŸ¯ Project Overview

This project implements a robust ensemble learning pipeline that combines multiple state-of-the-art machine learning models to predict heart disease probability. The solution includes:

- **Advanced Feature Engineering**: Creates interaction features and categorical transformations
- **Ensemble Learning**: Combines XGBoost, CatBoost, LightGBM, and Logistic Regression
- **Cross-Validation**: 10-fold stratified cross-validation for robust evaluation
- **Hyperparameter Optimization**: Optuna-based automatic hyperparameter tuning

## ğŸ“ Project Structure

```
heart-disease-prediction/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py                 # Configuration and hyperparameters
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_preprocessing.py     # Data loading and preprocessing
â”‚   â”œâ”€â”€ model_training.py         # Model training and ensemble
â”‚   â”œâ”€â”€ eda.py                    # Exploratory data analysis
â”‚   â”œâ”€â”€ hyperparameter_tuning.py  # Optuna-based hyperparameter tuning
â”‚   â”œâ”€â”€ train.py                  # Main training pipeline
â”‚   â””â”€â”€ predict.py                # Prediction on new data
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                 # Training data
â”‚   â”œâ”€â”€ test.csv                  # Test data
â”‚   â””â”€â”€ sample_submission.csv     # Submission template
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb         # Jupyter notebook for exploration
â”‚
â”œâ”€â”€ models/                       # Saved model files
â”œâ”€â”€ outputs/                      # Results and submissions
â”œâ”€â”€ tests/                        # Unit tests
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- pip

### Installation

1. **Clone or download the project**

2. **Install dependencies**:
```bash
cd heart-disease-prediction
pip install -r requirements.txt
```

3. **Place your data files** in the `data/` directory:
   - `train.csv`
   - `test.csv`
   - `sample_submission.csv`

## ğŸ’» Usage

### 1. Basic Training

Train the ensemble model with default settings:

```bash
python src/train.py
```

### 2. Training with EDA

Perform exploratory data analysis before training:

```bash
python src/train.py --eda
```

### 3. Custom Output Directory

Specify a custom output directory:

```bash
python src/train.py --output-dir ./my_results
```

### 4. Hyperparameter Tuning

For hyperparameter optimization (requires more time):

```python
from src.hyperparameter_tuning import tune_hyperparameters
from src.data_preprocessing import get_data

X, y, X_test, _ = get_data()
best_params = tune_hyperparameters(X, y, n_trials=100)
```

### 5. Making Predictions on New Data

```bash
python src/predict.py data/new_data.csv --output predictions.csv
```

## ğŸ”§ Configuration

All model parameters and settings can be customized in `config/config.py`:

```python
# Adjust cross-validation folds
N_SPLITS = 10

# Modify ensemble weights
ENSEMBLE_WEIGHTS = {
    'xgboost': 0.35,
    'catboost': 0.35,
    'lightgbm': 0.20,
    'logistic': 0.10
}

# Update model hyperparameters
XGBOOST_PARAMS = {
    'n_estimators': 500,
    'max_depth': 6,
    'learning_rate': 0.05,
    # ... more parameters
}
```

## ğŸ“Š Features

### Data Preprocessing
- Automatic handling of categorical variables
- RobustScaler for outlier-resistant normalization
- Feature engineering:
  - Age groups
  - Cholesterol risk categories
  - Blood pressure categories
  - HR percentage of age-predicted max
  - Interaction features (AgeÃ—BP, AgeÃ—Cholesterol, BPÃ—Cholesterol)

### Models

1. **XGBoost**: Gradient boosting with extreme optimization
2. **CatBoost**: Gradient boosting optimized for categorical features
3. **LightGBM**: Fast gradient boosting framework
4. **Logistic Regression**: Linear baseline model

### Ensemble Strategy

- **Weighted Averaging**: Combines predictions from all models using optimized weights
- **Stratified K-Fold CV**: Ensures balanced class distribution across folds
- **Out-of-Fold Predictions**: Generates unbiased validation predictions

## ğŸ“ˆ Performance Metrics

The model is evaluated using:
- **ROC-AUC Score**: Primary metric for model comparison
- **Accuracy**: Overall prediction accuracy
- **Precision**: Positive prediction accuracy
- **Recall**: True positive rate
- **F1-Score**: Harmonic mean of precision and recall

## ğŸ” Key Improvements Over Original Notebook

1. âœ… **Fixed Data Leakage**: Scaler fitted only on training data
2. âœ… **Modular Code Structure**: Clean, reusable components
3. âœ… **Enhanced Feature Engineering**: Additional meaningful features
4. âœ… **More Models**: Added LightGBM to ensemble
5. âœ… **Better Cross-Validation**: Increased to 10 folds
6. âœ… **Hyperparameter Optimization**: Optuna integration
7. âœ… **Comprehensive Logging**: Detailed training progress
8. âœ… **Configuration Management**: Centralized settings
9. âœ… **OOF Predictions**: Saved for model analysis
10. âœ… **Production-Ready**: Modular and deployable

## ğŸ“ Example Output

```
================================================================================
TRAINING ENSEMBLE WITH 10-FOLD CROSS-VALIDATION
================================================================================

Fold 1/10
----------------------------------------
  xgboost      - ROC-AUC: 0.89234
  catboost     - ROC-AUC: 0.89456
  lightgbm     - ROC-AUC: 0.88923
  logistic     - ROC-AUC: 0.86234

  Ensemble     - ROC-AUC: 0.89678

...

================================================================================
CROSS-VALIDATION RESULTS
================================================================================

Individual Model OOF ROC-AUC Scores:
  xgboost     : 0.89123
  catboost    : 0.89345
  lightgbm    : 0.88834
  logistic    : 0.86123

Weighted Ensemble OOF ROC-AUC: 0.89567
================================================================================
```

## ğŸ§ª Testing

Run unit tests:

```bash
python -m pytest tests/
```

## ğŸ“¦ Dependencies

Key libraries:
- `scikit-learn`: Machine learning utilities
- `xgboost`: Gradient boosting
- `catboost`: Gradient boosting for categorical features
- `lightgbm`: Fast gradient boosting
- `optuna`: Hyperparameter optimization
- `pandas`: Data manipulation
- `numpy`: Numerical computing
- `matplotlib/seaborn`: Visualization

## ğŸ¤ Contributing

Feel free to submit issues and enhancement requests!

## ğŸ“„ License

This project is provided as-is for educational and research purposes.

## ğŸ‘¥ Author

Created for heart disease prediction using advanced machine learning techniques.

## ğŸ™ Acknowledgments

- Original Kaggle notebook inspiration
- Scikit-learn and ensemble learning community
- XGBoost, CatBoost, and LightGBM developers

---

**Happy Predicting! ğŸ«€ğŸ’»**
